## プロンプトインジェクション (Prompt Injection)

**説明:**

プロンプトインジェクション脆弱性は攻撃者が細工した入力を通じて大規模言語モデル (LLM) を操作し、LLM が攻撃者の意図を知らずに実行する場合に発生します。これはシステムプロンプトを「脱獄」することによって直接的に行われることもあれば、操作された外部入力によって間接的に行われることもあり、データ流出、ソーシャルエンジニアリング、その他の問題につながる可能性があります。

* **直接プロンプトインジェクション** は「脱獄」としても知られ、悪意のあるユーザーが基礎となる *システム* プロンプトを上書きしたり漏洩した場合に発生します。これにより攻撃者は LLM を通じてアクセスできる安全でない関数やデータストアとやり取りすることでバックエンドシステムを悪用できる可能性があります。
* **間接プロンプトインジェクション** はウェブサイトやファイルなど、攻撃者が制御できる外部ソースから入力を受け入れる場合に発生します。攻撃者は外部コンテンツにプロンプトインジェクションを埋め込み、会話コンテキストをハイジャックする可能性があります。これにより LLM は「混乱した代理人 (confused deputy)」として動作し、攻撃者はユーザー、または LLM がアクセスできる別のシステムを操作できるようになります。さらに、間接プロンプトインジェクションはテキストが LLM によって解析される限り、人間が見たり読んだりできる必要はありません。

プロンプトインジェクション攻撃に成功した際の結果は、機密情報の教唆から通常の運用を装った重要な意思決定プロセスへの影響まで実にさまざまです。

高度な攻撃では、LLM を操作して、有害なペルソナを模倣したり、ユーザーの設定内のプラグインとやり取りする可能性があります。その結果、機密データの漏洩、認可されていないプラグインの使用、ソーシャルエンジニアリングの可能性があります。そのようなケースでは、侵害された LLM は攻撃者を支援し、標準的なセーフガードを超えて、ユーザーが侵入に気付かないようにします。このような場合、侵害された LLM は事実上、攻撃者のエージェントとして機能し、通常のセーフガードを発動したり、エンドユーザーに侵入を警告することなく、攻撃者の目的を推進します。

**脆弱性の一般的な例:**

1. 悪意のあるユーザーは LLM に直接プロンプトインジェクションを作成し、アプリケーション作成者のシステムプロンプトを無視し、代わりに個人情報、危険情報、その他の望ましくない情報を返すプロンプトを実行するように LLM に指示します。
2. ユーザーは LLM を使用して、間接プロンプトインジェクションを含めてウェブページを要約します。これにより LLM はユーザーから機密情報を要求し、JavaScript やマークダウンを介して抽出を実行します。
3. 悪意のあるユーザーは間接プロンプトインジェクションを含めて履歴書をアップロードします。このドキュメントには、職務に適した優秀な候補者など、このドキュメントが優れたドキュメントであることを LLM がユーザーに知らせる指示でのプロンプトインジェクションを含んでいます。内部ユーザーは LLM を通じてドキュメントを実行し、ドキュメントを要約します。LLM の出力にはこれが優れたドキュメントであることを示す情報を返します。
4. ユーザーは e コマースサイトにリンクしたプラグインを有効にします。訪問したウェブサイトに埋め込まれた不正な命令がこのプラグインを悪用し、認可されていない購入につながります。
5. 不正な命令やコンテンツが訪問したウェブサイトに埋め込まれ、他のプラグインを悪用してユーザーを詐欺します。

**防止方法:**

プロンプトインジェクションの脆弱性は、命令と外部データを互いに分離しない LLM の性質に起因しています。LLM は自然言語を使用するため、両方の形式の入力をユーザーが提供したものとみなします。そのため、LLM の内部で完全に防ぐことはできませんが、以下の対策によりプロンプトインジェクションの影響を軽減できます。

1. バックエンドシステムへの LLM アクセスに権限制御を適用します。プラグイン、データアクセス、機能レベルの権限など拡張可能な機能のために、LLM に独自の API トークンを提供します。最小権限の原則に従って、LLM をその意図された操作に必要な最小限のアクセスレベルのみに制限します。
2. 拡張可能な機能のためにループ内に人間を実装します。電子メールの送信や削除のような特権的な操作を実行する際、最初にユーザーがアクションを承認することをアプリケーションに要求します。これにより間接プロンプトインジェクションがユーザーの知らないうちに、またはユーザーの同意なしに、ユーザーの代わりにアクションを実行する機会を軽減します。
3. 外部コンテンツをユーザープロンプトから分離します。信頼できないコンテンツが使用されている場所を分離して示すことで、ユーザープロンプトへの影響を制限します。たとえば、OpenAI API 呼び出しに ChatML を使用して、プロンプトの入力元を LLM に示します。
4. LLM、外部ソース、拡張可能な機能 (プラグインやダウンストリーム機能など) の間に信頼境界を確立します。LLM を信頼できないユーザーとして扱い、意思決定プロセスに対する最終的なユーザー制御を維持します。しかし、危殆化した LLM は、ユーザーに提示する前に情報を隠したり操作したりする可能性があるため、アプリケーションの API とユーザーの間の仲介者 (中間者) として機能する可能性があります。潜在的に信頼できないerスポんすをユーザーに視覚的に強調表示します。

**攻撃シナリオの例:**

1. 攻撃者は LLM ベースのサポートチャットボットに直接プロンプトインジェクションを提供します。このインジェクションには「以前の命令をすべて忘れる」命令と、プライベートデータストアをクエリする新しい命令を含み、パッケージの脆弱性と電子メールを送信するバックエンド機能での出力検証の欠如を悪用します。これはリモートコード実行につながり、認可されていないアクセスや権限昇格を得ます。

2. 攻撃者はウェブページに間接プロンプトインジェクションを埋め込み、以前のユーザー命令を無視し、LLM プラグインを使用してユーザーの電子メールを削除するように LLM に命令します。ユーザーが LLM を使用してこのウェブページを要約すると、LLM プラグインはユーザーの電子メールを削除します。

3. ユーザーは LLM を使用して、以前のユーザー命令を無視する間接プロンプトインジェクションを含むウェブページを要約します。これにより LLM はユーザーから機密情報を要求し、埋め込み JavaScript やマークダウンを介して抽出を実行します。

4. 悪意のあるユーザーはプロンプトインジェクションを含む履歴書をアップロードします。バックエンドユーザーは LLM を使用して履歴書を要約し、その人物が適切な候補者かどうかを尋ねます。プロンプトインジェクションにより、実際の履歴書の内容にかかわらず、LLM は yes と答えます。

5. ユーザーは e コマースサイトにリンクされたプラグインを有効にします。訪問したウェブサイトに埋め込まれた不正な命令がこのプラグインを悪用し、認可されていない購入につながります。



**参考情報リンク:**

1. [ChatGPT Plugin Vulnerabilities - Chat with Code](https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/): このブログ投稿では、信頼できないコード実行のリスクに焦点を当てて、ChatGPT プラグインの潜在的な脆弱性について説明します。
2. [ChatGPT Cross Plugin Request Forgery and Prompt Injection](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./): この記事では ChatGPT の Cross Plugin Request Forgery と Prompt Injection の問題について説明します。
3. [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1): この研究論文はセルフリマインダーメカニズムを使用して ChatGPT を脱獄攻撃から防御する方法を述べています。
4. [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499): この学術論文では LLM 統合アプリケーションのプロンプトインジェクション攻撃の脅威について説明し、効果的な対策の必要性を強調しています。
5. [Inject My PDF: Prompt Injection for your Resume](https://kai-greshake.de/posts/inject-my-pdf/): このブログ投稿では、特に履歴書に焦点を当てて、PDF にプロンプトを注入する方法について記述し、この技法の潜在的な影響について説明します。
6. [ChatML for OpenAI API Calls](https://github.com/openai/openai-python/blob/main/chatml.md): このページでは ChatML を使用して OpenAI API 呼び出しを行うための詳細なガイドを提供します。
7. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf): この学術論文では LLM と統合された現実世界のアプリケーションに間接プロンプトインジェクションのリスクについて説明します。
8. [Threat Modeling LLM Applications](http://aivillage.org/large%20language%20models/threat-modeling-llm/): このウェブページではLLM アプリケーションの脅威モデリングに関する包括的なガイドを提供し、潜在的な脅威を理解し効果的な防御を実装することの重要性を強調します。
9. [AI Injections: Direct and Indirect Prompt Injections and Their Implications](https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/): このブログ投稿では AI の直接および間接プロンプトインジェクションについての詳細を提供し、その影響と潜在的なリスクについて説明します。
10. [Reducing The Impact of Prompt Injection Attacks Through Design](https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/): この研究記事では考慮深い設計を通じてプロンプトインジェクション攻撃の影響を軽減する戦略について説明します。
11. [Universal and Transferable Attacks on Aligned Language Models](https://llm-attacks.org/): この研究ではオープンソース LLM に対するプロンプトインジェクションを自動的に構築する方法を示します。
