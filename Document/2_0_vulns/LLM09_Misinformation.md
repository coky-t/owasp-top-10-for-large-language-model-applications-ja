## LLM09:2025 誤情報 (Misinformation)

### 説明

LLM からの誤情報は、これらのモデルに依存するアプリケーションにとって重大な脆弱性となります。誤情報は、LLM が信頼できるように見える誤った情報や誤解を招く情報を生成する際に発生します。この脆弱性はセキュリティ侵害、風評被害、法的責任につながる可能性があります。

誤情報の主な原因の一つは幻覚で、LLM が正確そうに見えるが捏造されたコンテンツを生成するものです。幻覚は、LLM がそのコンテンツを真に理解せずに統計パターンを使用してトレーニングデータのギャップを埋める際に発生します。結果として、モデルは正しいように聞こえるがまったく根拠のない回答を生成する可能性があります。幻覚は誤情報の主な原因ですが、それだけが原因ではありません。トレーニングデータによってもたらされるバイアスや不完全な情報も一因となることもあります。

関連する問題に過度の依存があります。過度の依存は、LLM が生成したコンテンツをユーザーが過度に信頼し、その正確性を検証できない場合に発生します。この過度の依存は誤情報の影響を悪化させます。ユーザは十分な精査を行わずに誤ったデータを重要な意思決定やプロセスに統合する可能性があるためです。

### リスクの一般的な例

#### 1. 事実の不正確さ
  モデルは正しくない発言を生成し、ユーザーが誤った情報に基づいて意思決定を行うことになります。たとえば、Air Canada のチャットボットは旅行者に誤情報を提供し、運航の混乱と法的な問題につながりました。結果としてその航空会社に勝訴しました。
  (参照リンク: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))
#### 2. 裏付けのない主張
  モデルは根拠のない主張を生成し、医療や法的手続きなどのセンシティブな状況では特に有害となる可能性があります。たとえば、ChatGPT は偽の訴訟事例を捏造し、法廷での重大な問題につながりました。
  (参照リンク: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))
#### 3. 専門知識の虚偽表示
  モデルは複雑なトピックを理解しているという錯覚を与え、専門知識のレベルに関してユーザーに誤解を与えます。たとえば、チャットボットは医療関連の問題の複雑さを虚偽表示し、存在しない不確実性を示唆し、裏付けのない治療法がまだ議論中であると考えられているとユーザーに誤解を与えます。
  (参照リンク: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))
#### 4. 安全でないコード生成
  モデルは安全でないコードライブラリや存在しないコードライブラリを提案し、ソフトウェアシステムに統合されると脆弱性をもたらす可能性があります。たとえば、LLM は安全でないサードパーティライブラリの使用を提案し、検証せずに信頼するとセキュリティリスクにつながります。
  (参照リンク: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### 予防および緩和戦略

#### 1. 検索拡張生成 (Retrieval-Augmented Generation, RAG)
  検索拡張生成を使用し、レスポンス生成時に信頼できる外部データベースから関連する検証済みの情報を取得することで、モデル出力の信頼性を高めます。これは幻覚や誤情報のリスクを緩和するのに役立ちます。
#### 2. モデルのファインチューニング
  ファインチューニングやエンベディングでモデルを強化し、出力品質を改善します。パラメータ効率的チューニング (Parameter-Efficient Tuning, PET) や思考連鎖 (Chain-of-Thought) プロンプトなどの技法は誤情報の発生を減らすのに役立ちます。
#### 3. 相互検証と人的監視
  ユーザーに LLM 出力を信頼できる外部ソースと相互チェックするように促し、情報の正確性を確保します。特に重要な情報や機密情報については、人的監視と事実確認プロセスを実装します。人間のレビュー担当者が適切に訓練されていることを確保し、AI 生成コンテンツへの過度の依存を避けます。
#### 4. 自動バリデーションメカニズム
  主要な出力、特に利害の大きい環境からの出力、を自動的に検証するツールとプロセスを導入します。
#### 5. リスクの伝達
  LLM が生成するコンテンツに関連するリスクと起こり得る弊害を特定し、誤情報の可能性を含め、これらのリスクと制限をユーザーに明確に伝えます。
#### 6. セキュアコーディングプラクティス
  セキュアコーディングプラクティスを確立して、誤ったコード提案による脆弱性の統合を防ぎます。
#### 7. ユーザーインタフェースの設計
  コンテンツフィルタの統合、AI 生成コンテンツの明確なラベル付け、信頼性と正確性の制限に関するユーザーへの通知など、LLM の責任ある使用を促す API とユーザーインタフェースを設計します。想定される使用分野の制限について具体的に示します。
#### 8. トレーニングと教育
  LLM の限界、生成されるコンテンツの独立した検証の重要性、批判的思考の必要性について、ユーザーに包括的なトレーニングを提供します。特定のコンテキストでは、ドメイン固有のトレーニングを提供して、ユーザーが専門分野内で LLM 出力を効果的に評価できるようにします。

### 攻撃シナリオの例

#### シナリオ #1
  攻撃者は、人気のあるコーディングアシスタントを試みて、よくある幻覚となるようなパッケージ名を探します。頻繁に提案されるが存在しないライブラリを特定すると、その名前で悪意のあるパッケージを広く使用されているリポジトリに公開します。開発者はコーディングアシスタントの提案に頼り、知らないうちにこれらの偽装パッケージソフトウェアに統合します。結果として、攻撃者は不正アクセスを行い、悪意のあるコードを注入したり、バックドアを確立して、重大なセキュリティ侵害やユーザーデータの危殆化につながります。
#### シナリオ #2
  企業は十分な正確さを確保していない医療診断用のチャットボットを提供します。そのチャットボットは質の悪い情報を提供し、患者に有害な結果をもたらします。結果として、損害に対してその企業に勝訴しました。このケースでは、安全性とセキュリティの崩壊には悪意のある攻撃者は必要ありませんが、代わりに LLM システムの不十分な監視と信頼性から生じました。このシナリオでは、企業に対する積極的な攻撃者は必要ありませんが、風評被害や金銭的損害のリスクがあります。

### 参考情報リンク

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### 関連するフレームワークと分類

インフラストラクチャデプロイメント、適用される環境制御、その他のベストプラクティスに関する包括的な情報、シナリオ戦略については、このセクションを参照してください。

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
