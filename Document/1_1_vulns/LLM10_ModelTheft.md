## LLM10: モデル窃取 (Model Theft)

### 説明

このエントリでは悪意のある攻撃者や APT による LLM モデルへの認可されていないアクセスや抽出について言及します。これはプロプライエタリ LLM モデル (貴重な知的財産である) が侵害されたり、物理的に盗まれたり、複製されたり、機能的に等価なものを作成するために重みとパラメータが抽出される場合に発生します。LLM モデル窃取の影響には、経済的およびブランド的評判の損失、競争上の優勢性の低下、モデルの認可されていない使用、モデル内に含まれる機密情報の認可されていないアクセスなどの可能性があります。

言語モデルがまずます強力になり普及するにつれて、LLM の窃取はセキュリティ上の重大な懸念事項となっています。組織や研究者は堅牢なセキュリティ対策を優先して、LLM モデルを保護し、知的財産の機密性と完全性を確保する必要があります。アクセス制御、暗号化、継続的監視を含む包括的なセキュリティフレームワークを採用することは、LLM モデル窃取に関連するリスクを軽減し、LLM に依存する個人と組織の両方の利益を守るうえで極めて重要です。

### 脆弱性の一般的な例

1. 攻撃者はある企業のインフラストラクチャの脆弱性を悪用して、ネットワークやアプリケーションセキュリティ設定の構成ミスを介して LLM モデルリポジトリへの認可されていないアクセスを獲得します。
2. 不満を抱いた従業員がモデルや関連成果物を漏洩する内部脅威シナリオ。
3. 攻撃者は注意深く作成された入力とプロンプトインジェクション技法を使用してモデル API をクエリし、十分な数の出力を収集して、シャドウモデルを作成します。
4. 悪意のある攻撃者は LLM の入力フィルタリング技法をバイパスして、サイドチャネル攻撃を実行し、最終的にモデルの重みとアーキテクチャ情報を遠隔操作されたリソースに収集できます。
5. モデル抽出のための攻撃ベクトルには、特定のトピックに関する多数のプロンプトで LLM をクエリすることが含まれます。LLM からの出力は別のモデルをファインチューニングするために使用できます。ただし、この攻撃については注意すべき点がいくつかあります。
   - 攻撃者はターゲットとなるプロンプトを多数生成しなければなりません。プロンプトは十分に具体的ではない場合、LLM からの出力は役に立ちません。
   - LLM からの出力には幻覚のような回答が含まれることがあります。つまり、出力の一部が無意味となる可能性があるため、攻撃者はモデル全体を抽出できないことがあります。
     - モデル抽出を通じて LLM を 100% 複製することは可能ではありません。ただし、攻撃者は部分的なモデルを複製できます。
6. **_機能モデルの複製_** の攻撃ベクトルは、プロンプトを介してターゲットモデルを使用して合成トレーニングデータを生成し ("self-instruct" と呼ばれるアプローチ) 、それを使用して別の基本モデルをファインチューニングし、機能的に同等なものを生成することが含まれます。これは例 5 で使用される従来のクエリベースの抽出の制限を回避し、LLM を使用して他の LLM を訓練する研究での使用に成功しています。この研究の文脈では、モデル複製は攻撃ではありません。このアプローチは攻撃者が使用して、パブリック API でプロプライエタリモデルを複製できる可能性があります。

盗まれたモデルをシャドウモデルとして使用すると、モデル内に含まれる機密情報への認可されていないアクセスなどの敵対的な攻撃を仕掛けたり、敵対的入力を検出されずにさらに高度なプロンプトインジェクションを実験したりできます。

### 予防および緩和戦略

1. 強力なアクセス制御 (RBAC や最小権限のルールなど) や強力な認証メカニズムを実装して、LLM モデルリポジトリとトレーニング環境への認可されていないアクセスを制限します。
   1. これは特に最初の三つの一般的な例に当てはまり、インサイダー脅威、構成ミスや、LLM モデル、重み、アーキテクチャを格納するインフラストラクチャに関する脆弱なセキュリティコントロールによって、この脆弱性を引き起こし、悪意のあるアクターが環境の内部または外部から侵入する可能性があります。
   2. サプライヤー管理の追跡、検証、依存関係の脆弱性はサプライチェーン攻撃の悪用を防ぐための重要な焦点トピックです。
2. ネットワークリソース、内部サービス、API への LLM のアクセスを制限します。
   1. これはインサイダーリスクと脅威をカバーするだけでなく、最終的に LLM アプリケーションが "*_アクセスできる_*" ものを制御するので、サイドチャネル攻撃を防ぐためのメカニズムまたは防止ステップになりえるため、すべての一般的な例に特に当てはまります。
3. 運用環境で使用される ML モデルには一元管理された ML モデルインベントリを使用します。一元管理されたモデルレジストリを持つことで、ガバナンスの優れた基盤となるアクセス制御、認証、監視/ログ記録機能を介して ML モデルへの認可されていないアクセスを防止できます。また、一元管理されたリポジトリを持つことで、コンプライアンス、リスク評価、リスク軽減の目的として、モデルで使用されるアルゴリズムに関するデータを収集するためにも有益です。
4. LLM モデルリポジトリに関連するアクセスログとアクティビティを定期的に監視および監査して、疑わしい行為や認可されていない行為を迅速に検出して対応します。
5. ガバナンスと追跡および承認ワークフローで MLOps デプロイメントを自動化し、インフラストラクチャ内のアクセスとデプロイメントのコントロールを強化します。
6. 管理策と緩和策を導入して、サイドチャネル攻撃を引き起こすプロンプトインジェクション技法のリスクを緩和ないし軽減します。
7. 適用可能であれば API 呼び出しのレート制限や、LLM アプリケーションからデータ流出のリスクを軽減するためのフィルター、他の監視システムからの (DLP などの) 抽出アクティビティを検出する技法を実装します。
8. 敵対的ロバストネストレーニングを実装して、抽出クエリを検出し、物理的なセキュリティ対策を強化します。
9. LLM ライフサイクルの埋め込みと検出のステージにウォーターマークのフレームワークを実装します。

### 攻撃シナリオの例

1. 攻撃者は企業のインフラストラクチャの脆弱性を悪用して、LLM モデルリポジトリへの認可されていないアクセスを獲得します。攻撃者は貴重な LLM モデルの抽出を進め、それらを使用して、競合する言語処理サービスを立ち上げたり機密情報を抽出して、元の企業に重大な経済的損害を与えます。
2. 不満を抱いた従業員はモデルや関連成果物を漏洩します。このシナリオが一般に公開されることで、攻撃者の知識が増加し、グレーボックスの敵対的攻撃や、利用可能な財産を直接盗むことが可能になります。
3. 攻撃者は注意深く選択した入力で API をクエリして、シャドウモデルを作成するのに十分な数の出力を収集します。
4. サプライチェーン内にセキュリティコントロール上の欠陥が存在して、プロプライエタリモデル情報のデータ漏洩につながります。
5. 悪意のある攻撃者は LLM の入力フィルタリング技法とプリアンブルを回避してサイドチャネル攻撃を実行して、制御下にある遠隔操作されたリソースにモデル情報を取得します。

### 参考情報リンク

1. [Meta’s powerful AI language model has leaked online](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse): **The Verge**
2. [Runaway LLaMA | How Meta's LLaMA NLP model leaked](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/): **Deep Learning Blog**
3. [AML.TA0000 ML Model Access](https://atlas.mitre.org/tactics/AML.TA0000): **MITRE ATLAS**
4. [I Know What You See:](https://arxiv.org/pdf/1803.05847.pdf): **Arxiv White Paper**
5. [D-DAE: Defense-Penetrating Model Extraction Attacks:](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a432/1He7YbsiH4c): **Computer.org**
6. [A Comprehensive Defense Framework Against Model Extraction Attacks](https://ieeexplore.ieee.org/document/10080996): **IEEE**
7. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): **Stanford Center on Research for Foundation Models (CRFM)**
8. [How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html): **KD Nuggets**
